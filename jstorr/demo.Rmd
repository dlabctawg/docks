---
title: "JSTORr"
output: html_notebook
---

```{r, warning=FALSE}
#! tutorial copied from: https://github.com/benmarwick/JSTORr
#! due to bug fixes note that we've installed from https://github.com/brooksambrose/JSTORr
# load the package to your current R session (do this every time you start R and use the functions)
library(JSTORr)
```

```{r}
# change the path to where you unzipped your file on your computer
unpack1grams <- JSTOR_unpack1grams(path = paste(getwd(),dir('dd',full.names = T),sep=.Platform$file.sep))
```

```{r}
unpack1grams
```

```{r}
#! year distribution
plot(table(unpack1grams$bibliodata$year))
# one word over time
JSTOR_1word(unpack1grams, "passion")
JSTOR_1word(unpack1grams, "family")
# two words over time
JSTOR_2words(unpack1grams,"passion","family",yearfrom=1900,yearto=2000)
# correlation of words over time
JSTOR_2wordcor(unpack1grams, "passion", "family",yearfrom=1900,yearto=2000)
```

```{r}
# subset the words to get nouns only
nouns <-  JSTOR_dtmofnouns(unpack1grams, sparse = 0.75)
# plot and tabulate the most frequent words over time
JSTOR_freqwords(unpack1grams, nouns)
# plot and tabulate words correlated with 'passion' over time, sadly broken :(
#JSTOR_findassocs(unpack1grams, nouns, "passion")
```

```{r}
# plot and tabulate clusters of documents that contain the word 'passion'
JSTOR_clusterbywords(nouns, 'passion')
# pretty tough to parse with a largish corpus!
```

```{r}
# generate topic model with 50 topics (an arbitrary choice)
my_model <- JSTOR_lda(unpack1grams, nouns, 50)
# visualise document distances by topics
JSTOR_lda_docdists(my_model)
# plot and tabulate hot and cold topics
JSTOR_lda_hotncoldtopics(my_model)

```

